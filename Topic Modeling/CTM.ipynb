{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:48:28.204418Z",
     "start_time": "2024-11-11T19:48:24.661899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import corpora\n",
    "import os\n",
    "import importlib\n",
    "from PopMusicInformationRetrieval import utility_functions as utils\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "id": "eaf48881963c5795",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:48:28.208084Z",
     "start_time": "2024-11-11T19:48:28.205478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_topics_list = [2, 3, 4]\n",
    "ctm_epochs_list = [5, 10, 20]\n",
    "ctm_learning_rates = [2e-3, 5e-3, 1e-2]\n",
    "ctm_batch_sizes = [64, 128]\n",
    "\n",
    "# Create all combinations of parameters\n",
    "ctm_param_grid = list(itertools.product(num_topics_list, ctm_epochs_list, ctm_learning_rates, ctm_batch_sizes))"
   ],
   "id": "dae93c2d889dcaf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:48:55.074480Z",
     "start_time": "2024-11-11T19:48:54.714761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load your dataframe\n",
    "df = pd.read_pickle('../Data/dataframes/preprocessed_df.pkl')\n",
    "\n",
    "if isinstance(df['Tokens'].iloc[0], str):\n",
    "    import ast\n",
    "    df['Tokens'] = df['Tokens'].apply(ast.literal_eval)\n",
    "\n",
    "texts = df['Tokens']\n",
    "texts_bow = [' '.join(tokens) for tokens in df['Tokens']]\n",
    "df['Lyrics'] = df['Lyrics'].apply(utils.light_preprocessing)\n",
    "documents = df['Lyrics']"
   ],
   "id": "652e2a994b70cf6f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:48:55.556440Z",
     "start_time": "2024-11-11T19:48:55.542493Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "d924c38df1b885bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Artist                                Album  \\\n",
       "0      Big L  Lifestylez Ov Da Poor and Dangerous   \n",
       "1      Big L  Lifestylez Ov Da Poor and Dangerous   \n",
       "2      Big L  Lifestylez Ov Da Poor and Dangerous   \n",
       "3      Big L  Lifestylez Ov Da Poor and Dangerous   \n",
       "4      Big L  Lifestylez Ov Da Poor and Dangerous   \n",
       "...      ...                                  ...   \n",
       "1363  Dr.Dre                          The Chronic   \n",
       "1364  Dr.Dre                          The Chronic   \n",
       "1365  Dr.Dre                          The Chronic   \n",
       "1366  Dr.Dre                          The Chronic   \n",
       "1367  Dr.Dre                          The Chronic   \n",
       "\n",
       "                                                   Song       Coast  \\\n",
       "0                                        8 Iz Enuff.mp3  east_coast   \n",
       "1                                      Da Graveyard.mp3  east_coast   \n",
       "2                             I Don't Understand It.mp3  east_coast   \n",
       "3                                 No Endz, No Skinz.mp3  east_coast   \n",
       "4                                               MVP.mp3  east_coast   \n",
       "...                                                 ...         ...   \n",
       "1363  Dr. Dre - The Day the Niggaz Took Over (feat. ...  west_coast   \n",
       "1364  Dr. Dre - Bitches Ain't Shit (feat. Jewell, Sn...  west_coast   \n",
       "1365  Dr. Dre - Stranded On Death Row (feat. Bushwic...  west_coast   \n",
       "1366  Dr. Dre - Nuthin' but a ＂G＂ Thang (feat. Snoop...  west_coast   \n",
       "1367  Dr. Dre - The Roach (ft. Ruben Cruz, Daz Dilli...  west_coast   \n",
       "\n",
       "      Release Year      Tempo1     Tempo2  Duration (s)  Sample Rate (Hz)  \\\n",
       "0             1995   96.774194  48.000000    298.840000             48000   \n",
       "1             1995   93.750000  46.511628    323.760000             48000   \n",
       "2             1995   93.750000  47.244094    260.226667             48000   \n",
       "3             1995  100.000000  50.420168    208.733333             48000   \n",
       "4             1995   86.956522  43.478261    218.866667             48000   \n",
       "...            ...         ...        ...           ...               ...   \n",
       "1363          1992   93.750000  46.875000    273.206000             48000   \n",
       "1364          1992   92.307692  46.153846    287.207625             48000   \n",
       "1365          1992   90.909091  45.801527    287.335333             48000   \n",
       "1366          1992   95.238095  47.244094    238.677917             48000   \n",
       "1367          1992   89.552239  44.117647    277.072125             48000   \n",
       "\n",
       "                                                   Path  \\\n",
       "0     /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "1     /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "2     /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "3     /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "4     /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "...                                                 ...   \n",
       "1363  /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "1364  /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "1365  /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "1366  /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "1367  /Users/borosabel/Documents/Uni/Thesis/PopMIR/D...   \n",
       "\n",
       "                                                 Lyrics  \\\n",
       "0     yo my crew is in the house terra herb mcgruff ...   \n",
       "1     it's the number one crew in the area big l be ...   \n",
       "2     there are too many mc's who are overrated you ...   \n",
       "3     let me get to the point real quick when ya poc...   \n",
       "4     a yo spark up the phillies and pass the stout ...   \n",
       "...                                                 ...   \n",
       "1363  i'ma say this and i'ma end mine if you ain't d...   \n",
       "1364  bitches ain't shit but hoes and tricks bitches...   \n",
       "1365  yes it is i \" says me and all who agree are mo...   \n",
       "1366  one two three and to the four snoop doggy dogg...   \n",
       "1367  cannabis sativa ha ha or in the heart of la kn...   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0     [crew, house, bless, big, mike, imma, set, fol...   \n",
       "1     [number, one, crew, big, nigga, men, win, kill...   \n",
       "2     [many, mcs, ask, even, supposed, make, rap, kn...   \n",
       "3     [let, point, real, quick, pocket, thick, mad, ...   \n",
       "4     [pass, make, quick, money, grip, ass, street, ...   \n",
       "...                                                 ...   \n",
       "1363  [say, end, mine, point, one, south, shit, need...   \n",
       "1364  [bitch, shit, hoe, trick, bitch, shit, hoe, tr...   \n",
       "1365  [yes, say, three, yes, house, sure, want, talk...   \n",
       "1366  [one, two, three, four, dog, dr, dre, door, re...   \n",
       "1367  [heart, la, know, even, though, six, million, ...   \n",
       "\n",
       "                                       Processed_Lyrics  \n",
       "0     crew house terra herb mcgruff buddah bless big...  \n",
       "1     number one crew area big lightin nigga incense...  \n",
       "2     many mcs overrated ask even supposed make rap ...  \n",
       "3     let get point real quick pocket thick mad chic...  \n",
       "4     spark phillies pass stout make quick money gri...  \n",
       "...                                                 ...  \n",
       "1363  say end mine africans united states period poi...  \n",
       "1364  bitch shit hoe trick bitch shit hoe trick lick...  \n",
       "1365  yes say agree three yes house sure want talk h...  \n",
       "1366  one two three four snoop doggy dog dr dre door...  \n",
       "1367  cannabis sativa heart la know chronic confused...  \n",
       "\n",
       "[1368 rows x 13 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Song</th>\n",
       "      <th>Coast</th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Tempo1</th>\n",
       "      <th>Tempo2</th>\n",
       "      <th>Duration (s)</th>\n",
       "      <th>Sample Rate (Hz)</th>\n",
       "      <th>Path</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Processed_Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big L</td>\n",
       "      <td>Lifestylez Ov Da Poor and Dangerous</td>\n",
       "      <td>8 Iz Enuff.mp3</td>\n",
       "      <td>east_coast</td>\n",
       "      <td>1995</td>\n",
       "      <td>96.774194</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>298.840000</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>yo my crew is in the house terra herb mcgruff ...</td>\n",
       "      <td>[crew, house, bless, big, mike, imma, set, fol...</td>\n",
       "      <td>crew house terra herb mcgruff buddah bless big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big L</td>\n",
       "      <td>Lifestylez Ov Da Poor and Dangerous</td>\n",
       "      <td>Da Graveyard.mp3</td>\n",
       "      <td>east_coast</td>\n",
       "      <td>1995</td>\n",
       "      <td>93.750000</td>\n",
       "      <td>46.511628</td>\n",
       "      <td>323.760000</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>it's the number one crew in the area big l be ...</td>\n",
       "      <td>[number, one, crew, big, nigga, men, win, kill...</td>\n",
       "      <td>number one crew area big lightin nigga incense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big L</td>\n",
       "      <td>Lifestylez Ov Da Poor and Dangerous</td>\n",
       "      <td>I Don't Understand It.mp3</td>\n",
       "      <td>east_coast</td>\n",
       "      <td>1995</td>\n",
       "      <td>93.750000</td>\n",
       "      <td>47.244094</td>\n",
       "      <td>260.226667</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>there are too many mc's who are overrated you ...</td>\n",
       "      <td>[many, mcs, ask, even, supposed, make, rap, kn...</td>\n",
       "      <td>many mcs overrated ask even supposed make rap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big L</td>\n",
       "      <td>Lifestylez Ov Da Poor and Dangerous</td>\n",
       "      <td>No Endz, No Skinz.mp3</td>\n",
       "      <td>east_coast</td>\n",
       "      <td>1995</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>50.420168</td>\n",
       "      <td>208.733333</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>let me get to the point real quick when ya poc...</td>\n",
       "      <td>[let, point, real, quick, pocket, thick, mad, ...</td>\n",
       "      <td>let get point real quick pocket thick mad chic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big L</td>\n",
       "      <td>Lifestylez Ov Da Poor and Dangerous</td>\n",
       "      <td>MVP.mp3</td>\n",
       "      <td>east_coast</td>\n",
       "      <td>1995</td>\n",
       "      <td>86.956522</td>\n",
       "      <td>43.478261</td>\n",
       "      <td>218.866667</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>a yo spark up the phillies and pass the stout ...</td>\n",
       "      <td>[pass, make, quick, money, grip, ass, street, ...</td>\n",
       "      <td>spark phillies pass stout make quick money gri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>Dr.Dre</td>\n",
       "      <td>The Chronic</td>\n",
       "      <td>Dr. Dre - The Day the Niggaz Took Over (feat. ...</td>\n",
       "      <td>west_coast</td>\n",
       "      <td>1992</td>\n",
       "      <td>93.750000</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>273.206000</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>i'ma say this and i'ma end mine if you ain't d...</td>\n",
       "      <td>[say, end, mine, point, one, south, shit, need...</td>\n",
       "      <td>say end mine africans united states period poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>Dr.Dre</td>\n",
       "      <td>The Chronic</td>\n",
       "      <td>Dr. Dre - Bitches Ain't Shit (feat. Jewell, Sn...</td>\n",
       "      <td>west_coast</td>\n",
       "      <td>1992</td>\n",
       "      <td>92.307692</td>\n",
       "      <td>46.153846</td>\n",
       "      <td>287.207625</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>bitches ain't shit but hoes and tricks bitches...</td>\n",
       "      <td>[bitch, shit, hoe, trick, bitch, shit, hoe, tr...</td>\n",
       "      <td>bitch shit hoe trick bitch shit hoe trick lick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>Dr.Dre</td>\n",
       "      <td>The Chronic</td>\n",
       "      <td>Dr. Dre - Stranded On Death Row (feat. Bushwic...</td>\n",
       "      <td>west_coast</td>\n",
       "      <td>1992</td>\n",
       "      <td>90.909091</td>\n",
       "      <td>45.801527</td>\n",
       "      <td>287.335333</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>yes it is i \" says me and all who agree are mo...</td>\n",
       "      <td>[yes, say, three, yes, house, sure, want, talk...</td>\n",
       "      <td>yes say agree three yes house sure want talk h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>Dr.Dre</td>\n",
       "      <td>The Chronic</td>\n",
       "      <td>Dr. Dre - Nuthin' but a ＂G＂ Thang (feat. Snoop...</td>\n",
       "      <td>west_coast</td>\n",
       "      <td>1992</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>47.244094</td>\n",
       "      <td>238.677917</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>one two three and to the four snoop doggy dogg...</td>\n",
       "      <td>[one, two, three, four, dog, dr, dre, door, re...</td>\n",
       "      <td>one two three four snoop doggy dog dr dre door...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>Dr.Dre</td>\n",
       "      <td>The Chronic</td>\n",
       "      <td>Dr. Dre - The Roach (ft. Ruben Cruz, Daz Dilli...</td>\n",
       "      <td>west_coast</td>\n",
       "      <td>1992</td>\n",
       "      <td>89.552239</td>\n",
       "      <td>44.117647</td>\n",
       "      <td>277.072125</td>\n",
       "      <td>48000</td>\n",
       "      <td>/Users/borosabel/Documents/Uni/Thesis/PopMIR/D...</td>\n",
       "      <td>cannabis sativa ha ha or in the heart of la kn...</td>\n",
       "      <td>[heart, la, know, even, though, six, million, ...</td>\n",
       "      <td>cannabis sativa heart la know chronic confused...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1368 rows × 13 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:50:52.015104Z",
     "start_time": "2024-11-11T19:48:58.118356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tp = TopicModelDataPreparation(\"all-mpnet-base-v2\", max_seq_length=512)\n",
    "training_dataset = tp.fit(text_for_contextual=documents, text_for_bow=texts_bow)"
   ],
   "id": "727caaf1bebe0cab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borosabel/lib/python3.9/site-packages/contextualized_topic_models/utils/data_preparation.py:64: UserWarning: the longest document in your collection has 1919 words, the model instead truncates to 512 tokens.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c9346b9614249cf81d8c133ff045ee3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:50:52.424377Z",
     "start_time": "2024-11-11T19:50:52.061691Z"
    }
   },
   "cell_type": "code",
   "source": "dictionary = corpora.Dictionary(texts)",
   "id": "8c113371d24b16e9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:56:42.281501Z",
     "start_time": "2024-11-11T19:56:30.048963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to evaluate CTM models and save results into a DataFrame\n",
    "def evaluate_ctm_models(training_dataset, tp, texts, dictionary, ctm_param_grid, metrics=('coherence',), save_dir='saved_models'):\n",
    "    \"\"\"\n",
    "    Evaluate CTM models with a given set of hyperparameters and metrics, and save results to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - training_dataset: The training dataset prepared by TopicModelDataPreparation.\n",
    "    - tp: The TopicModelDataPreparation object with vocabulary info.\n",
    "    - texts: The list of tokenized texts.\n",
    "    - dictionary: The Gensim dictionary.\n",
    "    - ctm_param_grid: List of tuples for hyperparameters (num_topics, epochs, learning_rate, batch_size).\n",
    "    - metrics: Tuple of metrics to evaluate ('coherence', 'diversity', or both).\n",
    "    - save_dir: Directory to save the results DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - results_df: A DataFrame containing the evaluation results.\n",
    "    \"\"\"\n",
    "    ctm_results = []\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Iterate through each parameter combination\n",
    "    for idx, (num_topics, epochs, learning_rate, batch_size) in enumerate(ctm_param_grid):\n",
    "        try:\n",
    "            print(f\"Training CTM model {idx+1}/{len(ctm_param_grid)} with num_topics={num_topics}, epochs={epochs}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
    "\n",
    "            # Initialize the model\n",
    "            ctm_model = CombinedTM(\n",
    "                bow_size=len(tp.vocab),\n",
    "                contextual_size=768,\n",
    "                n_components=num_topics,\n",
    "                num_epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                activation='softplus',\n",
    "                dropout=0.2,\n",
    "                solver='adam',\n",
    "                num_data_loader_workers=0,\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            ctm_model.fit(training_dataset)\n",
    "\n",
    "            # Get topics\n",
    "            ctm_topics = ctm_model.get_topic_lists(10)\n",
    "\n",
    "            # Initialize result dictionary\n",
    "            result = {\n",
    "                'model_id': idx + 1,\n",
    "                'num_topics': num_topics,\n",
    "                'epochs': epochs,\n",
    "                'learning_rate': learning_rate,\n",
    "                'batch_size': batch_size\n",
    "            }\n",
    "\n",
    "            # Evaluate the model based on specified metrics\n",
    "            if 'coherence' in metrics:\n",
    "                coherence_model_ctm = CoherenceModel(\n",
    "                    topics=ctm_topics,\n",
    "                    texts=texts,\n",
    "                    dictionary=dictionary,\n",
    "                    coherence='c_v'\n",
    "                )\n",
    "                coherence_ctm = coherence_model_ctm.get_coherence()\n",
    "                result['coherence_score'] = coherence_ctm\n",
    "\n",
    "            if 'diversity' in metrics:\n",
    "                unique_words = set()\n",
    "                total_words = 0\n",
    "\n",
    "                for topic in ctm_topics:\n",
    "                    unique_words.update(topic)\n",
    "                    total_words += len(topic)\n",
    "\n",
    "                topic_diversity = len(unique_words) / total_words if total_words > 0 else 0\n",
    "                result['topic_diversity'] = topic_diversity\n",
    "\n",
    "            # Save the result\n",
    "            ctm_results.append(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while training model {idx+1}: {e}\")\n",
    "            continue  # Skip this iteration if there's an error\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    results_df = pd.DataFrame(ctm_results)\n",
    "\n",
    "    # Sort by coherence_score if it is one of the metrics\n",
    "    if 'coherence' in metrics:\n",
    "        results_df = results_df.sort_values(by='coherence_score', ascending=False)\n",
    "\n",
    "    # Save results DataFrame for future reference\n",
    "    results_df_path = os.path.join(save_dir, 'ctm_model_results_summary.csv')\n",
    "    results_df.to_csv(results_df_path, index=False)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Example Usage\n",
    "results_df = evaluate_ctm_models(\n",
    "    training_dataset=training_dataset,\n",
    "    tp=tp,\n",
    "    texts=texts,\n",
    "    dictionary=dictionary,\n",
    "    ctm_param_grid=ctm_param_grid,\n",
    "    metrics=('coherence', 'diversity'),\n",
    "    save_dir='saved_models'\n",
    ")"
   ],
   "id": "53d4dcca10d0e83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTM model 1/54 with num_topics=2, epochs=5, learning_rate=0.002, batch_size=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5/5]\t Seen Samples: [6720/6840]\tTrain Loss: 1241.2576788039435\tTime: 0:00:00.200692: : 5it [00:01,  4.34it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 145.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTM model 2/54 with num_topics=2, epochs=5, learning_rate=0.002, batch_size=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5/5]\t Seen Samples: [6400/6840]\tTrain Loss: 1261.4403564453125\tTime: 0:00:00.152729: : 5it [00:00,  6.53it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 88.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTM model 3/54 with num_topics=2, epochs=5, learning_rate=0.005, batch_size=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5/5]\t Seen Samples: [6720/6840]\tTrain Loss: 1248.3433779761904\tTime: 0:00:00.246352: : 5it [00:01,  4.62it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 161.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTM model 4/54 with num_topics=2, epochs=5, learning_rate=0.005, batch_size=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5/5]\t Seen Samples: [6400/6840]\tTrain Loss: 1258.0240844726563\tTime: 0:00:00.142985: : 5it [00:00,  6.85it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 96.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTM model 5/54 with num_topics=2, epochs=5, learning_rate=0.01, batch_size=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [5/5]\t Seen Samples: [6720/6840]\tTrain Loss: 1244.5738641648065\tTime: 0:00:00.200665: : 5it [00:01,  4.54it/s]\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 100\u001B[0m\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results_df\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# Example Usage\u001B[39;00m\n\u001B[0;32m--> 100\u001B[0m results_df \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_ctm_models\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdictionary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdictionary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mctm_param_grid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctm_param_grid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcoherence\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdiversity\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msaved_models\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[1;32m    108\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 43\u001B[0m, in \u001B[0;36mevaluate_ctm_models\u001B[0;34m(training_dataset, tp, texts, dictionary, ctm_param_grid, metrics, save_dir)\u001B[0m\n\u001B[1;32m     30\u001B[0m ctm_model \u001B[38;5;241m=\u001B[39m CombinedTM(\n\u001B[1;32m     31\u001B[0m     bow_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(tp\u001B[38;5;241m.\u001B[39mvocab),\n\u001B[1;32m     32\u001B[0m     contextual_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m768\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m     num_data_loader_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     40\u001B[0m )\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m \u001B[43mctm_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Get topics\u001B[39;00m\n\u001B[1;32m     46\u001B[0m ctm_topics \u001B[38;5;241m=\u001B[39m ctm_model\u001B[38;5;241m.\u001B[39mget_topic_lists(\u001B[38;5;241m10\u001B[39m)\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/contextualized_topic_models/models/ctm.py:421\u001B[0m, in \u001B[0;36mCTM.fit\u001B[0;34m(self, train_dataset, validation_dataset, save_dir, verbose, patience, delta, n_samples, do_train_predictions)\u001B[0m\n\u001B[1;32m    419\u001B[0m pbar\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    420\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_train_predictions:\n\u001B[0;32m--> 421\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_doc_topic_distributions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_doc_topic_distribution\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_samples\u001B[49m\n\u001B[1;32m    423\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/contextualized_topic_models/models/ctm.py:515\u001B[0m, in \u001B[0;36mCTM.get_doc_topic_distribution\u001B[0;34m(self, dataset, n_samples)\u001B[0m\n\u001B[1;32m    513\u001B[0m final_thetas \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 515\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_samples \u001B[38;5;129;01min\u001B[39;00m tqdm(loader):\n\u001B[1;32m    516\u001B[0m         \u001B[38;5;66;03m# batch_size x vocab_size\u001B[39;00m\n\u001B[1;32m    517\u001B[0m         X_bow \u001B[38;5;241m=\u001B[39m batch_samples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX_bow\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    518\u001B[0m         X_bow \u001B[38;5;241m=\u001B[39m X_bow\u001B[38;5;241m.\u001B[39mreshape(X_bow\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/tqdm/std.py:1182\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1179\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1182\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/contextualized_topic_models/datasets/dataset.py:33\u001B[0m, in \u001B[0;36mCTMDataset.__getitem__\u001B[0;34m(self, i)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return sample from dataset at index i.\"\"\"\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_bow[i]) \u001B[38;5;241m==\u001B[39m scipy\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39mcsr_matrix:\n\u001B[0;32m---> 33\u001B[0m     X_bow \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFloatTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mX_bow\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtodense\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m     X_contextual \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX_contextual[i])\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:46:45.081525Z",
     "start_time": "2024-11-11T19:46:45.066437Z"
    }
   },
   "cell_type": "code",
   "source": "results_df",
   "id": "1453e523195612ad",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mresults_df\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:57:02.785826Z",
     "start_time": "2024-11-11T19:57:02.725664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_df = pd.read_csv('../saved_models/ctm_model_results_summary.csv')\n",
    "best_params = results_df.iloc[0]  # Assuming the first row has the best score after sorting\n",
    "\n",
    "# Extract hyperparameters\n",
    "best_num_topics = int(best_params['num_topics'])\n",
    "best_epochs = int(best_params['epochs'])\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_batch_size = int(best_params['batch_size'])\n",
    "\n",
    "best_ctm_model = CombinedTM(\n",
    "    bow_size=len(tp.vocab),\n",
    "    contextual_size=768,\n",
    "    n_components=best_num_topics,\n",
    "    num_epochs=best_epochs,\n",
    "    batch_size=best_batch_size,\n",
    "    activation='softplus',\n",
    "    dropout=0.2,\n",
    "    solver='adam',\n",
    "    num_data_loader_workers=0,\n",
    ")\n",
    "\n",
    "# Train the model using the original training dataset\n",
    "best_ctm_model.fit(training_dataset)"
   ],
   "id": "cb4ca3b027caccce",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../saved_models/ctm_model_results_summary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m results_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../saved_models/ctm_model_results_summary.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m best_params \u001B[38;5;241m=\u001B[39m results_df\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# Assuming the first row has the best score after sorting\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Extract hyperparameters\u001B[39;00m\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    936\u001B[0m     dialect,\n\u001B[1;32m    937\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    945\u001B[0m )\n\u001B[1;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/lib/python3.9/site-packages/pandas/io/common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../saved_models/ctm_model_results_summary.csv'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T19:46:49.202867Z",
     "start_time": "2024-11-11T19:46:49.180108Z"
    }
   },
   "cell_type": "code",
   "source": "best_ctm_model.get_topics()",
   "id": "2ce1e8773e98ef81",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_ctm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbest_ctm_model\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'best_ctm_model' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e36a029d8180b99f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
